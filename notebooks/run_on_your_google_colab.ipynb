{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cdc7594",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e95c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from logging import INFO, StreamHandler, getLogger\n",
    "\n",
    "logger = getLogger()\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(StreamHandler(sys.stdout))\n",
    "logger.setLevel(INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import copy\n",
    "import dataclasses\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "import typing\n",
    "from functools import partial\n",
    "from typing import Callable, Iterable, List, Literal, Optional, Sequence, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import xarray as xr\n",
    "from einops import rearrange\n",
    "from torch import Tensor, nn\n",
    "from torch.amp import GradScaler\n",
    "from torch.distributions import Categorical, Normal\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.style.use(\"tableau-colorblind10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0100a",
   "metadata": {},
   "source": [
    "# Examine SDE and diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53645229",
   "metadata": {},
   "source": [
    "- 理論ノートで扱ってきた線形 SDE はしばしば Ornstein-Uhlenbeck 過程 (OU 過程) と呼ばれる [wikipedia](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)\n",
    "$$\n",
    "dx = -\\mu x dt + \\sigma dW\n",
    "$$\n",
    "- 初期分布を混合正規分布に取る (二峰分布)\n",
    "- そして時間発展を SDE を解くことで求める"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9d7ed",
   "metadata": {},
   "source": [
    "## Run SDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a0cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed Gaussian\n",
    "init_means = [4.0, -3.0]\n",
    "init_stds = [2.0, 1.0]\n",
    "gauss_weights = [0.6, 0.4]\n",
    "\n",
    "b = 2_000  # n_batches, number of paths\n",
    "n_steps = 1_000\n",
    "dt = 1.0 / n_steps\n",
    "\n",
    "# OU process: dx = -mu x dt + sigma dW\n",
    "mu = 5.0\n",
    "sigma = np.sqrt(2.0 * mu)\n",
    "\n",
    "idxs = Categorical(torch.tensor(gauss_weights)).sample((b,))\n",
    "y0 = torch.normal(\n",
    "    mean=torch.tensor(init_means)[idxs], std=torch.tensor(init_stds)[idxs]\n",
    ")\n",
    "dW = math.sqrt(dt) * torch.randn(size=(b, n_steps))\n",
    "\n",
    "yt = torch.zeros((b, n_steps + 1))\n",
    "current = y0\n",
    "yt[:, 0] = current.detach().clone()\n",
    "\n",
    "for i in range(n_steps):\n",
    "    current = current - mu * current * dt + sigma * dW[:, i]\n",
    "    yt[:, i + 1] = current.detach().clone()\n",
    "\n",
    "ts = torch.linspace(0, 1, n_steps + 1)\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "fig = plt.plot()\n",
    "ax = plt.subplot()\n",
    "\n",
    "interval = 4\n",
    "for i in range(100):\n",
    "    ax.plot(ts[::interval], yt[i][::interval], lw=0.5, alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(r\"Time, $t$\")\n",
    "ax.set_ylabel(r\"State, $x$\")\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "fig, axes = plt.subplots(2, 6, sharex=True, sharey=True, figsize=(20, 6))\n",
    "\n",
    "for t, ax in zip([0, 1, 2, 5, 10, 20, 50, 100, 200, 500, 750, 1000], axes.flatten()):\n",
    "    data = yt[:, t].numpy().flatten()\n",
    "    ax.hist(data, range=(-10, 10), bins=100, density=True, alpha=1.0)\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_title(f\"Timestep={t:04}/{n_steps}\")\n",
    "    ax.set_ylabel(\"PDF\")\n",
    "    ax.axvline(0, color=\"gray\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del y0, yt, ts, mu, sigma, idxs\n",
    "del init_means, init_stds, gauss_weights\n",
    "del n_steps, dt, current, b, interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a5950",
   "metadata": {},
   "source": [
    "- 二峰を持つ確率分布が，単峰の標準正規分布に発展する様子が確認される"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a8d62",
   "metadata": {},
   "source": [
    "## Define diffusion framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791efdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass()\n",
    "class DDPMConfig:\n",
    "    start_beta: float\n",
    "    end_beta: float\n",
    "    n_timesteps: int\n",
    "    n_channels: int\n",
    "    n_spaces: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DDPMConfig,\n",
    "        neural_net: nn.Module,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dtype = torch.float32\n",
    "        self.device = device\n",
    "        self.c = copy.deepcopy(config)\n",
    "        self.net = neural_net\n",
    "        self._set_noise_schedule()\n",
    "\n",
    "    def _set_noise_schedule(self):\n",
    "        to_torch = partial(torch.tensor, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        betas = _make_beta_schedule(\n",
    "            schedule=\"linear\",\n",
    "            start=self.c.start_beta,\n",
    "            end=self.c.end_beta,\n",
    "            n_timesteps=self.c.n_timesteps,\n",
    "        )\n",
    "        times = np.linspace(\n",
    "            0.0, 1.0, num=len(betas) + 1, endpoint=True, dtype=np.float64\n",
    "        )\n",
    "        times = times[1:]  # skip the initial value\n",
    "        assert len(times) == len(betas) == self.c.n_timesteps\n",
    "\n",
    "        self.dt = 1.0 / float(self.c.n_timesteps)\n",
    "        self.sqrt_dt = math.sqrt(self.dt)\n",
    "\n",
    "        # variance-preserving SDE\n",
    "        frictions = 0.5 * betas\n",
    "        sigmas = np.sqrt(betas)\n",
    "\n",
    "        decays, vars = _precompute_ou(mu=frictions, sigma=sigmas, dt=self.dt)\n",
    "        stds = np.sqrt(vars)\n",
    "        # the OU solution is expressed as x_t = decay * x_0 + std * epsilon (epsilon ~ N(0,1))\n",
    "\n",
    "        # the number of elements in each param is equal to self.c.n_timesteps\n",
    "        self.register_buffer(\"frictions\", to_torch(frictions))\n",
    "        self.register_buffer(\"sigmas\", to_torch(sigmas))\n",
    "        self.register_buffer(\"times\", to_torch(times))\n",
    "\n",
    "        # Register params except for the initial values because std is initially zero\n",
    "        # Later, std is used as denominator to convert noise into the score function.\n",
    "        self.register_buffer(\"decays\", to_torch(decays[1:]))\n",
    "        self.register_buffer(\"stds\", to_torch(stds[1:]))\n",
    "\n",
    "        assert (\n",
    "            self.frictions.shape\n",
    "            == self.sigmas.shape\n",
    "            == self.times.shape\n",
    "            == self.decays.shape\n",
    "            == self.stds.shape\n",
    "            == (self.c.n_timesteps,)\n",
    "        )\n",
    "        assert torch.all(self.sigmas > 0.0) and torch.all(self.stds > 0.0)\n",
    "\n",
    "    def _extract_params(\n",
    "        self, params: torch.Tensor, t_indices: torch.Tensor, for_broadcast: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        def select(array):\n",
    "            return torch.index_select(array, dim=0, index=t_indices)\n",
    "            # Select diffusion times along batch dim\n",
    "\n",
    "        (n_batches,) = t_indices.shape\n",
    "\n",
    "        selected = select(params)\n",
    "        assert selected.shape == (n_batches,)\n",
    "\n",
    "        # add channel and space dims\n",
    "        if for_broadcast:\n",
    "            return selected.requires_grad_(False)[:, None, None]\n",
    "        else:\n",
    "            return selected.requires_grad_(False)\n",
    "\n",
    "    def _forward_sample_y(\n",
    "        self, y0: torch.Tensor, t_index: torch.Tensor, noise: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        #\n",
    "        a = self._extract_params(self.decays, t_index)\n",
    "        b = self._extract_params(self.stds, t_index)\n",
    "        return a * y0 + b * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _backward_sample_y(\n",
    "        self,\n",
    "        yt: torch.Tensor,\n",
    "        t_index: torch.Tensor,\n",
    "        y_cond: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        friction = self._extract_params(self.frictions, t_index)\n",
    "        sigma = self._extract_params(self.sigmas, t_index)\n",
    "        std = self._extract_params(self.stds, t_index)\n",
    "        t = self._extract_params(self.times, t_index, for_broadcast=False)\n",
    "        t = t[:, None]  # add channel dim\n",
    "\n",
    "        est_noise = self.net(yt=yt, y_cond=y_cond, t=t, t_index=t_index)\n",
    "        score = -est_noise / std\n",
    "\n",
    "        mean = yt + self.dt * (friction * yt + (sigma**2) * score)\n",
    "        dW = self.sqrt_dt * torch.randn_like(yt)\n",
    "\n",
    "        n_batches = yt.shape[0]\n",
    "        mask = (1 - (t_index == 0).float()).reshape(n_batches, *((1,) * (yt.ndim - 1)))\n",
    "        mask = mask.to(dtype=self.dtype, device=self.device)\n",
    "        # no noise at t_index == 0\n",
    "\n",
    "        return mean + mask * sigma * dW\n",
    "\n",
    "    # public methods\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def backward_sample_y(\n",
    "        self,\n",
    "        n_batches: int,\n",
    "        y_cond: Optional[torch.Tensor] = None,\n",
    "        n_return_steps: Optional[int] = None,\n",
    "        tqdm_disable: bool = False,\n",
    "    ) -> dict[int, torch.Tensor]:\n",
    "        assert not self.net.training\n",
    "\n",
    "        size = (n_batches, self.c.n_channels, self.c.n_spaces)\n",
    "        yt = torch.randn(size=size, device=self.device)\n",
    "        yt = self.stds[-1] * yt\n",
    "\n",
    "        if n_return_steps is not None:\n",
    "            interval = self.c.n_timesteps // n_return_steps\n",
    "\n",
    "        intermidiates: dict[int, torch.Tensor] = {}\n",
    "\n",
    "        for i in tqdm(\n",
    "            reversed(range(0, self.c.n_timesteps)),\n",
    "            total=self.c.n_timesteps,\n",
    "            disable=tqdm_disable,\n",
    "        ):\n",
    "            if interval is not None and (i + 1) % interval == 0:\n",
    "                intermidiates[i + 1] = yt.detach().clone().cpu()\n",
    "\n",
    "            index = torch.full((n_batches,), i, device=self.device, dtype=torch.long)\n",
    "            yt = self._backward_sample_y(yt=yt, y_cond=y_cond, t_index=index)\n",
    "\n",
    "        intermidiates[0] = yt.detach().clone().cpu()\n",
    "\n",
    "        return intermidiates\n",
    "\n",
    "    def forward(\n",
    "        self, y0: torch.Tensor, y_cond: Optional[torch.Tensor] = None, **kwargs\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        assert y0.ndim == 3  # batch, channel, space\n",
    "        assert y0.shape[1] == self.c.n_channels\n",
    "        assert y0.shape[2] == self.c.n_spaces\n",
    "\n",
    "        b = y0.shape[0]\n",
    "        t_index = torch.randint(0, self.c.n_timesteps, (b,), device=self.device).long()\n",
    "\n",
    "        noise = torch.randn_like(y0)\n",
    "\n",
    "        yt = self._forward_sample_y(y0=y0, t_index=t_index, noise=noise)\n",
    "        t = self._extract_params(self.times, t_index, for_broadcast=False)\n",
    "        t = t[:, None]  # add channel dim\n",
    "        noise_hat = self.net(yt=yt, y_cond=y_cond, t=t, t_index=t_index)\n",
    "\n",
    "        return noise, noise_hat\n",
    "\n",
    "\n",
    "def _make_beta_schedule(\n",
    "    schedule: str,\n",
    "    start: float,\n",
    "    end: float,\n",
    "    n_timesteps: int,\n",
    ") -> np.ndarray:\n",
    "    if schedule == \"linear\":\n",
    "        betas = np.linspace(start, end, n_timesteps, dtype=np.float64, endpoint=True)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Not supported: {schedule=}\")\n",
    "    return betas\n",
    "\n",
    "\n",
    "def _precompute_ou(\n",
    "    mu: np.ndarray,\n",
    "    sigma: np.ndarray,\n",
    "    dt: float | np.ndarray,\n",
    "    init_variance: float = 0.0,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Method to compute the mean and variance for OU process.\n",
    "    OU process: dx = -mu x dt + sigma dW\n",
    "    \"\"\"\n",
    "    mu = np.array(mu, dtype=np.float64)\n",
    "    assert np.all(mu >= 0.0)\n",
    "\n",
    "    sigma = np.array(sigma, dtype=np.float64)\n",
    "    assert np.all(sigma >= 0.0)\n",
    "\n",
    "    if isinstance(dt, float):\n",
    "        dt = np.full_like(mu, dt, dtype=np.float64)\n",
    "    else:\n",
    "        dt = np.array(dt, dtype=np.float64)\n",
    "    assert mu.shape == sigma.shape == dt.shape\n",
    "    assert init_variance >= 0.0\n",
    "\n",
    "    N = mu.size\n",
    "    m = np.empty(N + 1, dtype=np.float64)  # mean\n",
    "    v = np.empty(N + 1, dtype=np.float64)  # variance\n",
    "    m[0] = 1.0\n",
    "    v[0] = init_variance\n",
    "\n",
    "    for n in range(N):\n",
    "        decay = np.exp(-mu[n] * dt[n])\n",
    "        m[n + 1] = decay * m[n]\n",
    "        if mu[n] == 0.0:\n",
    "            q = sigma[n] ** 2 * dt[n]\n",
    "        else:\n",
    "            q = sigma[n] ** 2 * (1.0 - decay**2) / (2.0 * mu[n])\n",
    "        v[n + 1] = decay**2 * v[n] + q\n",
    "\n",
    "    return np.array(m), np.array(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b5efd",
   "metadata": {},
   "source": [
    "## Make a diffusion model instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379875a3",
   "metadata": {},
   "source": [
    "- 分散保存型 (Variance-Preserving; VP) の拡散モデルを扱う．\n",
    "- 順過程は下の様に書ける\n",
    "$$\n",
    "\\begin{align}\n",
    "dx_t &= - \\mu_t x \\; dt + \\sqrt{2\\mu_t} \\; dW \\quad (t \\in [0,1]) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "- 混合正規分布の FPE の解は解析的に書ける\n",
    "$$\n",
    "\\begin{align}\n",
    "  p(x,0) &= \\sum w_i {\\cal N}(x; m_i(0), s_i^2(0)) \\\\\n",
    "  p(x,t) &= \\sum w_i {\\cal N}(x; m_i(t), s_i^2(t)) \\\\\n",
    "  m_i(t) &= m_i(0) e^{-\\mu t} \\\\\n",
    "  s_i^2(t) &= s_i^2(0) e^{-2\\mu t} + (1 - e^{-2\\mu t})\n",
    "\\end{align}\n",
    "$$\n",
    "- この結果を利用して，厳密にスコア関数を与える\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\text{score function} &= \\frac{\\partial}{\\partial x} \\ln p(x,t) \\\\\n",
    "  &= -\\frac{1}{\\sum w_i {\\cal N}(x; m_i(t), s_i^2(t))} \\sum_i \\left[ w_i {\\cal N}(x; m_i(t), s_i^2(t)) \\frac{x-m_i(t)}{s_i^2(t)} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "- スコア関数が厳密に分かれば，逆 SDE の積分が可能になる\n",
    "- 拡散モデルのフレームワークに厳密なスコア関数を与えるインスタンスを代入している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU process: dx = -mu x dt + sigma dW\n",
    "mu = 5.0\n",
    "sigma = np.sqrt(2.0 * mu)\n",
    "\n",
    "# Mixed Gaussian\n",
    "init_means = [4.0, -3.0]\n",
    "init_stds = [2.0, 1.0]\n",
    "gauss_weights = [0.6, 0.4]\n",
    "\n",
    "config = DDPMConfig(\n",
    "    start_beta=2 * mu,  # 始点と終点の beta の設定値を一定にすると，上の mu も一定となる\n",
    "    end_beta=2 * mu,  # mu = beta / 2 (i.e., beta = 2 mu) の関係あり\n",
    "    n_timesteps=1_000,\n",
    "    n_channels=1,\n",
    "    n_spaces=1,\n",
    ")\n",
    "\n",
    "\n",
    "class ExactScoreFunc(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mu: float,\n",
    "        sigma: float,\n",
    "        n_t: int,\n",
    "        init_means: list[float],\n",
    "        init_stds: list[float],\n",
    "        gauss_weights: list[float],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        times = torch.linspace(0, 1, n_t + 1, dtype=torch.float32)\n",
    "\n",
    "        # 線形 SDE (OU 過程) の解より得られる分散などの解析形\n",
    "        # 分散保存系の設定だと sigma**2 / (2 * mu) == 1 になる\n",
    "        self.m_t = torch.exp(-mu * times)[1:]\n",
    "        self.v_t = (sigma**2 / (2 * mu) * (1.0 - torch.exp(-2 * mu * times)))[1:]\n",
    "        self.s_t = torch.sqrt(self.v_t)\n",
    "\n",
    "        assert len(init_means) == len(init_stds) == len(gauss_weights)\n",
    "        self.init_means = torch.tensor(init_means)\n",
    "        self.init_stds = torch.tensor(init_stds)\n",
    "        self.gauss_weights = torch.tensor(gauss_weights)\n",
    "\n",
    "    def potential(self, yt: torch.Tensor, t_index: int):\n",
    "        assert yt.ndim == 3  # batch, channel, and space\n",
    "        m_t = torch.index_select(self.m_t, dim=0, index=t_index)[:, None, None]\n",
    "        v_t = torch.index_select(self.v_t, dim=0, index=t_index)[:, None, None]\n",
    "        # add channel and space dims\n",
    "\n",
    "        # Calculate prob for the mixed Gaussian\n",
    "        probs = torch.zeros_like(yt)\n",
    "        for i in range(self.init_means.shape[0]):\n",
    "            mean = self.init_means[i] * m_t\n",
    "            var = self.init_stds[i] ** 2 * m_t**2 + v_t\n",
    "            p = torch.exp(-0.5 * (yt - mean) ** 2 / var) / torch.sqrt(2 * math.pi * var)\n",
    "            probs += self.gauss_weights[i] * p\n",
    "\n",
    "        return -torch.log(probs)\n",
    "\n",
    "    def score(self, yt: torch.Tensor, t_index: int) -> torch.Tensor:\n",
    "        assert yt.ndim == 3  # batch, channel, and space\n",
    "        m_t = torch.index_select(self.m_t, dim=0, index=t_index)[:, None, None]\n",
    "        v_t = torch.index_select(self.v_t, dim=0, index=t_index)[:, None, None]\n",
    "        # add channel and space dims\n",
    "\n",
    "        # Calculate prob for the mixed Gaussian\n",
    "        probs = torch.zeros_like(yt)\n",
    "        for i in range(self.init_means.shape[0]):\n",
    "            mean = self.init_means[i] * m_t\n",
    "            var = self.init_stds[i] ** 2 * m_t**2 + v_t\n",
    "            p = torch.exp(-0.5 * (yt - mean) ** 2 / var) / torch.sqrt(2 * math.pi * var)\n",
    "            probs += self.gauss_weights[i] * p\n",
    "\n",
    "        scores = torch.zeros_like(yt)\n",
    "        for i in range(self.init_means.shape[0]):\n",
    "            mean = self.init_means[i] * m_t\n",
    "            var = self.init_stds[i] ** 2 * m_t**2 + v_t\n",
    "            p = torch.exp(-0.5 * (yt - mean) ** 2 / var) / torch.sqrt(2 * math.pi * var)\n",
    "            scores = scores - (self.gauss_weights[i] * p / probs) * (yt - mean) / var\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def forward(self, yt: torch.Tensor, t_index: int, **kwargs) -> torch.Tensor:\n",
    "        assert yt.ndim == 3  # batch, channel, and space\n",
    "        scores = self.score(yt, t_index)\n",
    "        s_t = torch.index_select(self.s_t, dim=0, index=t_index)[:, None, None]\n",
    "        # add channel and space dims\n",
    "        return (-s_t * scores).to(torch.float32)  # estimate noise\n",
    "\n",
    "\n",
    "score_network = ExactScoreFunc(\n",
    "    mu=mu,\n",
    "    sigma=sigma,\n",
    "    n_t=config.n_timesteps,\n",
    "    init_means=init_means,\n",
    "    init_stds=init_stds,\n",
    "    gauss_weights=gauss_weights,\n",
    ")\n",
    "\n",
    "ddpm = DDPM(config=config, neural_net=score_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72b708",
   "metadata": {},
   "source": [
    "## Check variance preserving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e23f7",
   "metadata": {},
   "source": [
    "- 分散保存型であることの確認\n",
    "$$\n",
    "{\\rm Var}\\left[x_t^2\\right]= {\\rm Var}\\left[x_0^2\\right]  + \\left(1 - e^{-2 \\mu t}\\right)\n",
    "$$\n",
    "- 第二項が拡散モデル内部で用いるノイズの標準偏差を表す\n",
    "- 初期データ $x_0$ の分散が 1 に規格化されていると仮定すれば，全分散は常に 1 となる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.plot(ddpm.decays, label=r\"${\\rm decay} = e^{-\\mu t}$\")\n",
    "plt.plot(ddpm.stds, label=r\"${\\rm std} = \\sqrt{1-e^{-2\\mu t}}$\")\n",
    "vars = ddpm.decays**2 + ddpm.stds**2\n",
    "plt.plot(vars, label=\"total vars\")\n",
    "plt.axhline(0.0, ls=\"--\")\n",
    "plt.xlabel(\"Num. of Time Steps\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "del vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd0a34c",
   "metadata": {},
   "source": [
    "## Forward process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f614a98a",
   "metadata": {},
   "source": [
    "- 順過程では，二峰の分布が単峰の標準ガウス分布へ緩和する\n",
    "- 緩和が 300 ステップ程度で完了し，残りの 300 - 1000 ステップではほとんど確率分布が変化していない\n",
    "- これは SDE の摩擦項により時間に関して指数関数的な緩和を施しているため"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb99dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 10_000\n",
    "idxs = Categorical(torch.tensor(gauss_weights)).sample((b,))\n",
    "y0 = torch.normal(\n",
    "    mean=torch.tensor(init_means)[idxs], std=torch.tensor(init_stds)[idxs]\n",
    ")[:, None, None]\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "fig, axes = plt.subplots(2, 6, sharex=True, sharey=True, figsize=(20, 6))\n",
    "for t, ax in zip(torch.arange(-1, config.n_timesteps + 1, 100), axes.flatten()):\n",
    "    if t == -1:\n",
    "        yt = y0.detach().clone()\n",
    "    else:\n",
    "        noise = torch.randn_like(y0)\n",
    "        yt = ddpm._forward_sample_y(\n",
    "            y0=y0,\n",
    "            t_index=torch.full((b,), fill_value=t, dtype=torch.long),\n",
    "            noise=noise,\n",
    "        )\n",
    "    data = yt[:, 0, 0].numpy().flatten()\n",
    "    ax.hist(data, range=(-10, 10), bins=100, density=True)\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_title(f\"t={t+1}\\nmean={np.mean(data):.2f},std={np.std(data):.2f}\")\n",
    "    ax.set_ylabel(\"PDF\")\n",
    "    ax.axvline(0, color=\"k\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del b, y0, yt, data, idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417c1c2",
   "metadata": {},
   "source": [
    "## Backward process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d596f",
   "metadata": {},
   "source": [
    "- 逆過程では，正規分布から始まり，混合正規分布へと発展させる\n",
    "- 混合正規分布の FPE の解は解析的に書ける\n",
    "- この結果を利用して，厳密にスコア関数を与えて，逆 SDE を解いている\n",
    "- 逆過程は，厳密に順過程の逆回しになる\n",
    "- そのため，二峰性が現れるのが $t$ が 0 に十分近づいてからになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm.net.eval()\n",
    "lst_img = ddpm.backward_sample_y(n_batches=10_000, n_return_steps=config.n_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 14\n",
    "fig, axes = plt.subplots(2, 6, sharex=True, sharey=True, figsize=(20, 6))\n",
    "\n",
    "for t, ax in zip([0, 1, 2, 5, 10, 20, 50, 100, 200, 500, 750, 1000], axes.flatten()):\n",
    "    data = lst_img[int(t)][:, 0, 0].numpy().flatten()\n",
    "    ax.hist(data, range=(-10, 10), bins=100, density=True, alpha=0.3)\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_title(f\"Timestep={t:04}/{config.n_timesteps}\")\n",
    "    ax.set_ylabel(\"PDF\")\n",
    "    ax.axvline(0, color=\"gray\")\n",
    "\n",
    "    for x in init_means:\n",
    "        ax.axvline(x, ls=\"--\", lw=0.5, alpha=0.5, color=\"gray\")\n",
    "\n",
    "    yt = torch.linspace(-10, 10, 101)[:, None, None]\n",
    "    if t < config.n_timesteps:\n",
    "        # add channel and space dim\n",
    "        dummy_t = torch.full(size=(101,), fill_value=t).long()\n",
    "        potential = score_network.potential(yt, dummy_t)\n",
    "    else:\n",
    "        potential = 0.5 * yt**2\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(yt.squeeze(), potential.squeeze(), ls=\"-\", lw=2.0, color=\"coral\")\n",
    "    ax2.set_ylim(0, 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ts = sorted(lst_img.keys())\n",
    "yt = torch.stack([lst_img[t][:100].squeeze() for t in ts], dim=1)\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "fig = plt.plot()\n",
    "ax = plt.subplot()\n",
    "\n",
    "interval = 4\n",
    "for i in range(100):\n",
    "    ax.plot(ts[::interval] / np.max(ts), yt[i][::interval], lw=0.5, alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(r\"Time, $t$\")\n",
    "ax.set_ylabel(r\"State, $x$\")\n",
    "\n",
    "plt.show\n",
    "\n",
    "del ts, yt, t, potential, dummy_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b611e047",
   "metadata": {},
   "source": [
    "- 上の図では確率分布のポテンシャル $U(x,t) = -\\ln p(x,t)$ を線で描いている\n",
    "- 各パスを調べてみると，単峰の正規分布から二峰の混合正規分布に分かれている\n",
    "- 逆 SDE を解いているため，時刻は $t=1.0$ から $t=0.0$ に向かっている\n",
    "- SDE は Euler-Maruyama 法で解いている"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096104be",
   "metadata": {},
   "source": [
    "# Make Lorenz96 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388083d",
   "metadata": {},
   "source": [
    "- Lorenz96 モデルは地球大気の東西運動を表す理想モデル [wikipedia](https://en.wikipedia.org/wiki/Lorenz_96_model)\n",
    "$$\n",
    "\\frac{dx_i}{dt} = (x_{i+1}-x_{i-2})x_{i-1} - x_i + F\n",
    "$$\n",
    "- $F$ は上の設定で $F=8$ としている．これはカオスレジームとして知られる典型的な設定\n",
    "- $i$ が一次元上の格子点位置を示す\n",
    "- 右辺の第一項が移流項，第二項が摩擦，第三項 $F$ が強制となる\n",
    "- 平衡解として，すべての $i$ に対して $x_i = F$ が存在し，これは $F$ が十分大きいと不安定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45d14c",
   "metadata": {},
   "source": [
    "## Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd660a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = pathlib.Path(\".\").resolve()\n",
    "DL_DATA_DIR = str(ROOT_DIR / \"data\" / \"DL_data\" / \"lorenz96\")\n",
    "os.makedirs(DL_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b602b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BATCHES = 5_000  # 作成するデータセット数\n",
    "N_SPACES = 32  # 空間の格子点数\n",
    "N_TIMES = 5_000  # 時間ステップ数\n",
    "\n",
    "FORCING = 8.0\n",
    "AMP_PERTURBATION = 0.01\n",
    "DT = 0.005\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6382657",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_lorenz96(x0: Tensor, forcing: float, n_steps: int, dt: float) -> Tensor:\n",
    "\n",
    "    assert isinstance(x0, Tensor) and x0.ndim == 2  # batch and space\n",
    "    assert isinstance(forcing, float)\n",
    "    assert isinstance(n_steps, int) and n_steps > 0\n",
    "    assert isinstance(dt, float) and dt > 0.0\n",
    "\n",
    "    current = x0.clone().detach()\n",
    "    states = [current.clone().detach()]\n",
    "\n",
    "    for _ in tqdm(range(n_steps)):\n",
    "        rhs = _lorenz96_rhs(x=current, forcing=forcing)\n",
    "        current = current + dt * rhs\n",
    "        states.append(current.clone().detach())\n",
    "\n",
    "    return torch.stack(states, dim=1).cpu()  # stack along time dim\n",
    "\n",
    "\n",
    "def _lorenz96_rhs(x: Tensor, forcing: float) -> Tensor:\n",
    "\n",
    "    a = x.roll(shifts=-1, dims=1)\n",
    "    b = x.roll(shifts=2, dims=1)\n",
    "    c = x.roll(shifts=1, dims=1)\n",
    "    dxdt = (a - b) * c - x + forcing\n",
    "\n",
    "    return dxdt\n",
    "\n",
    "\n",
    "def set_seeds(seed: int = 42, use_deterministic: bool = True) -> None:\n",
    "    try:\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "\n",
    "        if use_deterministic:\n",
    "            torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception as e:\n",
    "        logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea0a52",
   "metadata": {},
   "source": [
    "## Integrate Lorenz96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c568a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "set_seeds(SEED)\n",
    "x0 = FORCING * torch.ones(size=(N_BATCHES, N_SPACES), dtype=dtype, device=device)\n",
    "x0 += torch.randn_like(x0) * AMP_PERTURBATION\n",
    "\n",
    "states = integrate_lorenz96(x0=x0, forcing=FORCING, n_steps=N_TIMES, dt=DT)\n",
    "\n",
    "if torch.any(torch.isnan(states)):\n",
    "    logger.warning(\"NaNs appear.\")\n",
    "elif torch.any(~torch.isfinite(states)):\n",
    "    logger.warning(\"Infs appear\")\n",
    "else:\n",
    "    logger.info(\"Integration was successfully finished.\")\n",
    "\n",
    "del x0, dtype, device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793a4293",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 14\n",
    "fig, axes = plt.subplots(1, 3, sharex=True, sharey=True, figsize=[10, 4])\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    d = states[i].numpy()[::40][\n",
    "        -32:\n",
    "    ]  ## 40 時間ステップごとに抽出し，最後の 32 要素を時間に沿って抽出\n",
    "    ts = np.arange(d.shape[0]) * 20 * DT\n",
    "    xs = np.linspace(0, 2 * math.pi, N_SPACES, endpoint=False)\n",
    "    X, T = np.meshgrid(xs, ts, indexing=\"ij\")\n",
    "    ret = ax.pcolormesh(\n",
    "        X, T, d.transpose(), vmin=-9, vmax=9, cmap=\"coolwarm\", shading=\"nearest\"\n",
    "    )\n",
    "    cbar = fig.colorbar(ret, ax=ax)\n",
    "    ax.set_xlabel(r\"Space, $x$\")\n",
    "    ax.set_ylabel(r\"Time, $t$\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del d, ts, xs, X, T, ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d22445",
   "metadata": {},
   "source": [
    "## Write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ae7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = states[:, ::40][:, -32:]\n",
    "## 40 時間ステップごとに抽出し，最後の 32 要素を時間に沿って抽出\n",
    "assert outs.shape == (N_BATCHES, 32, N_SPACES), f\"{outs.shape=}\"\n",
    "\n",
    "ts = np.arange(0.0, outs.shape[1]) * (DT * 40)\n",
    "xs = np.linspace(0, 2 * math.pi, N_SPACES, endpoint=False)\n",
    "\n",
    "da = xr.DataArray(\n",
    "    outs.numpy().astype(np.float32),\n",
    "    dims=[\"batch\", \"time\", \"space\"],\n",
    "    coords={\n",
    "        \"batch\": np.arange(N_BATCHES, dtype=np.int32),\n",
    "        \"time\": ts.astype(np.float32),\n",
    "        \"space\": xs.astype(np.float32),\n",
    "    },\n",
    "    name=\"lorenz96_trajectory\",\n",
    "    attrs={\n",
    "        \"forcing\": FORCING,\n",
    "        \"dt\": DT,\n",
    "        \"seed\": SEED,\n",
    "        \"amp_perturb\": AMP_PERTURBATION,\n",
    "        \"output_time_interval\": 40,\n",
    "    },\n",
    ")\n",
    "\n",
    "p = f\"{DL_DATA_DIR}/lorenz96_v00.nc\"\n",
    "da.to_netcdf(path=p)\n",
    "\n",
    "del states, outs, ts, xs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9714ff",
   "metadata": {},
   "source": [
    "# Tran diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf26f01",
   "metadata": {},
   "source": [
    "- 分散保存型 (Variance-Preserving; VP) の拡散モデルを扱う．\n",
    "- 順過程は下の様に書ける\n",
    "$$\n",
    "\\begin{align}\n",
    "dx_t &= -\\frac{1}{2} \\beta_t x \\; dt + \\sqrt{\\beta_t} \\; dW \\quad (t \\in [0,1]) \\\\\n",
    "\\beta_t &= \\beta_{\\rm start} + t \\beta_{\\rm end}\n",
    "\\end{align}\n",
    "$$\n",
    "- 理論ノートに合わせるため，$\\beta_{\\rm start}=\\beta_{\\rm end}$ に設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ac252",
   "metadata": {},
   "source": [
    "## Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(f\"{DEVICE=}\")\n",
    "ROOT_DIR = pathlib.Path(\".\").resolve()\n",
    "DL_DATA_DIR = str(ROOT_DIR / \"data\" / \"DL_data\" / \"lorenz96\")\n",
    "DL_DATA_FILE = str(ROOT_DIR / \"data\" / \"DL_data\" / \"lorenz96\" / \"lorenz96_v00.nc\")\n",
    "DL_RESULT_DIR = str(ROOT_DIR / \"data\" / \"DL_model\" / \"lorenz96_v00\")\n",
    "os.makedirs(DL_RESULT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806afab",
   "metadata": {},
   "source": [
    "## Make config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass()\n",
    "class ExperimentLorenz96Config:\n",
    "    batch_size: int\n",
    "    loss_name: str\n",
    "    learning_rate: float\n",
    "    #\n",
    "    n_features: int\n",
    "    list_channel: list[int]\n",
    "    #\n",
    "    total_epochs: int\n",
    "    save_interval: int\n",
    "    use_auto_mix_precision: bool\n",
    "    ddpm: DDPMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69987a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExperimentLorenz96Config(\n",
    "    # Training settings\n",
    "    batch_size=100,\n",
    "    loss_name=\"L2\",\n",
    "    learning_rate=1e-3,\n",
    "    total_epochs=40,\n",
    "    save_interval=4,\n",
    "    use_auto_mix_precision=False,\n",
    "    # For U-Net\n",
    "    n_features=32,\n",
    "    list_channel=[1, 2, 4],\n",
    "    # For DDPM\n",
    "    ddpm=DDPMConfig(\n",
    "        start_beta=1e1,  # start_beta == end_beta の設定により，beta を定数にする\n",
    "        end_beta=1e1,\n",
    "        n_timesteps=1_000,\n",
    "        n_channels=32,  # データの時間ステップ数をチャネル数として指定\n",
    "        n_spaces=32,  # 空間格子点数．dont change n_channels and n_spaces\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01480e45",
   "metadata": {},
   "source": [
    "## Make dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLorenz96(Dataset):\n",
    "    def __init__(self, path_to_dataarray: str):\n",
    "\n",
    "        self.data = xr.load_dataarray(path_to_dataarray)\n",
    "        assert self.data.dims == (\"batch\", \"time\", \"space\")\n",
    "\n",
    "        self.n_batch, self.n_times, self.n_spaces = self.data.shape\n",
    "        self.mean = self.data.mean().item()\n",
    "        self.std = self.data.std().item()\n",
    "\n",
    "        self.dtype = torch.float32\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]  # batch dimension\n",
    "\n",
    "    def standardize(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def standardize_inversely(self, data):\n",
    "        return data * self.std + self.mean\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict[str, torch.Tensor]:\n",
    "        data = self.data[idx].values  # time x space\n",
    "        standardized = self.standardize(data)\n",
    "        ret = torch.tensor(standardized, dtype=self.dtype)\n",
    "        assert ret.shape == (self.n_times, self.n_spaces)\n",
    "        return {\"y0\": ret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=DatasetLorenz96(DL_DATA_FILE),\n",
    "    batch_size=config.batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e348cde",
   "metadata": {},
   "source": [
    "### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c98b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataloader))[\"y0\"]\n",
    "assert data.shape == (\n",
    "    config.batch_size,\n",
    "    config.ddpm.n_channels,\n",
    "    config.ddpm.n_spaces,\n",
    ")  # batch, time (=channel), space dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b687137",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 14\n",
    "fig, axes = plt.subplots(1, 3, sharex=True, sharey=True, figsize=[10, 4])\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    d = dataloader.dataset.standardize_inversely(data[i].numpy())\n",
    "    ts = np.arange(d.shape[0]) * 0.2  # dt = 0.2\n",
    "    xs = np.linspace(0, 2 * math.pi, 32, endpoint=False)\n",
    "    X, T = np.meshgrid(xs, ts, indexing=\"ij\")\n",
    "    ret = ax.pcolormesh(\n",
    "        X, T, d.transpose(), vmin=-9, vmax=9, cmap=\"coolwarm\", shading=\"nearest\"\n",
    "    )\n",
    "    cbar = fig.colorbar(ret, ax=ax)\n",
    "    ax.set_xlabel(r\"Space, $x$\")\n",
    "    ax.set_ylabel(r\"Time, $t$\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "del d, ts, xs, X, T, ret, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e32c6",
   "metadata": {},
   "source": [
    "## Define unet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ee39c",
   "metadata": {},
   "source": [
    "### Define blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5589a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Downsample1D(\n",
    "    dim: int, kernel_size: int = 4, padding_mode: str = \"zeros\"\n",
    ") -> nn.Module:\n",
    "    return nn.Conv1d(\n",
    "        dim,\n",
    "        dim,\n",
    "        kernel_size=(kernel_size,),\n",
    "        stride=(kernel_size // 2,),\n",
    "        padding=(1,),\n",
    "        padding_mode=padding_mode,\n",
    "    )\n",
    "\n",
    "\n",
    "def Upsample1D(dim: int, kernel_size: int = 4) -> nn.Module:\n",
    "    return nn.ConvTranspose1d(\n",
    "        dim, dim, kernel_size=(kernel_size,), stride=(kernel_size // 2,), padding=(1,)\n",
    "    )\n",
    "\n",
    "\n",
    "def PeriodicDownsample1D(dim: int, kernel_size: int) -> nn.Module:\n",
    "    assert kernel_size % 2 == 1, \"kernel_size should be odd.\"\n",
    "    return nn.Conv1d(\n",
    "        dim,\n",
    "        dim,\n",
    "        kernel_size=(kernel_size,),\n",
    "        stride=(2,),\n",
    "        padding=((kernel_size - 1) // 2,),\n",
    "        padding_mode=\"circular\",\n",
    "    )\n",
    "\n",
    "\n",
    "class PeriodicUpsampleConv1d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int,\n",
    "        kernel_size: int,\n",
    "        out_ch: Optional[int] = None,\n",
    "        scale: int = 2,\n",
    "    ):\n",
    "        assert kernel_size % 2 == 1, \"kernel_size should be odd.\"\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.pad = (kernel_size - 1) // 2\n",
    "\n",
    "        out_ch = in_ch if out_ch is None else out_ch\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=scale, mode=\"nearest-exact\")\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_ch,\n",
    "            out_ch,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=self.pad,\n",
    "            padding_mode=\"circular\",\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98775c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm1D(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.scale = dim**0.5\n",
    "        self.gamma = nn.Parameter(torch.ones(dim, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Normalize along the channel dimension\n",
    "        return F.normalize(x, dim=1) * self.scale * self.gamma\n",
    "\n",
    "\n",
    "class FiLMBlock1D(nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int, dim_out: int, padding_mode: str):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv1d(\n",
    "            dim, dim_out, kernel_size=3, padding=1, padding_mode=padding_mode\n",
    "        )\n",
    "        self.norm = RMSNorm1D(dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        scale_shift: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if scale_shift is not None:\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "class ResnetBlock1D(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        dim_out: int,\n",
    "        padding_mode: str,\n",
    "        *,\n",
    "        time_emb_dim: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))\n",
    "            if time_emb_dim is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.block1 = FiLMBlock1D(dim, dim_out, padding_mode=padding_mode)\n",
    "        self.block2 = FiLMBlock1D(dim_out, dim_out, padding_mode=padding_mode)\n",
    "        self.res_conv = (\n",
    "            nn.Conv1d(dim, dim_out, kernel_size=1) if dim != dim_out else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, time_emb: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        scale_shift = None\n",
    "        if self.mlp is not None:\n",
    "            assert time_emb is not None\n",
    "            emb: torch.Tensor = self.mlp(time_emb)\n",
    "            scale_shift = rearrange(emb, \"b c -> b c 1\").chunk(2, dim=1)\n",
    "\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52021fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int, time_base: float):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.time_base = time_base\n",
    "        logger.info(f\"SinusoidalTimeEmbedding: {self.time_base=}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(self.time_base, device=device)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37613f7b",
   "metadata": {},
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c51a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet1D(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        padding_mode: Literal[\"zeros\", \"circular\"] = \"zeros\",\n",
    "        dim_mults: Sequence[int] = (1, 2, 4, 8),\n",
    "        init_dim: Optional[int] = None,\n",
    "        init_kernel_size: int = 5,\n",
    "        time_base: float = 1000.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        init_dim = dim if init_dim is None else init_dim\n",
    "        assert isinstance(init_dim, int)\n",
    "        assert init_kernel_size % 2 == 1, \"init kernel size must be odd\"\n",
    "\n",
    "        init_padding = init_kernel_size // 2\n",
    "        self.init_conv = nn.Conv1d(\n",
    "            in_channels,\n",
    "            init_dim,\n",
    "            kernel_size=init_kernel_size,\n",
    "            padding=init_padding,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        time_dim = dim * 4\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalTimeEmbedding(dim, time_base),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        self.downs: Iterable[nn.Module] = nn.ModuleList([])\n",
    "        self.ups: Iterable[nn.Module] = nn.ModuleList([])\n",
    "\n",
    "        num_resolutions = len(in_out)\n",
    "        block_class = ResnetBlock1D\n",
    "        block_class_cond = partial(\n",
    "            block_class, time_emb_dim=time_dim, padding_mode=padding_mode\n",
    "        )\n",
    "\n",
    "        Downsample: Callable[[int], nn.Module] = Downsample1D\n",
    "        if padding_mode == \"circular\":\n",
    "            logger.info(\"PeriodicDownsample1D is used.\")\n",
    "            Downsample = partial(PeriodicDownsample1D, kernel_size=5)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_class_cond(dim_in, dim_out),\n",
    "                        block_class_cond(dim_out, dim_out),\n",
    "                        (Downsample(dim_out) if not is_last else nn.Identity()),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_class_cond(mid_dim, mid_dim)\n",
    "        self.mid_block2 = block_class_cond(mid_dim, mid_dim)\n",
    "\n",
    "        Upsample: nn.Module | Callable[[int], nn.Module] = Upsample1D\n",
    "        if padding_mode == \"circular\":\n",
    "            logger.info(\"PeriodicUpsampleConv1d is used.\")\n",
    "            Upsample = partial(PeriodicUpsampleConv1d, kernel_size=5)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_class_cond(dim_out * 2, dim_in),\n",
    "                        block_class_cond(dim_in, dim_in),\n",
    "                        Upsample(dim_in) if not is_last else nn.Identity(),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.final_conv = nn.Sequential(\n",
    "            block_class(dim * 2, dim, padding_mode=padding_mode),\n",
    "            nn.Conv1d(dim, out_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        yt: torch.Tensor,\n",
    "        y_cond: torch.Tensor,  # not used\n",
    "        t_index: torch.Tensor,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        # x shape = b c h\n",
    "        # time shape = b\n",
    "\n",
    "        yt = self.init_conv(yt)\n",
    "        r = yt.clone()\n",
    "        t_index = self.time_mlp(t_index)\n",
    "\n",
    "        h: List[torch.Tensor] = []\n",
    "\n",
    "        for downs in self.downs:\n",
    "            assert isinstance(downs, nn.ModuleList)\n",
    "            block1, block2, downsample = downs\n",
    "            yt = block1(yt, t_index)\n",
    "            yt = block2(yt, t_index)\n",
    "            h.append(yt)\n",
    "            yt = downsample(yt)\n",
    "\n",
    "        yt = self.mid_block1(yt, t_index)\n",
    "        yt = self.mid_block2(yt, t_index)\n",
    "\n",
    "        for ups in self.ups:\n",
    "            assert isinstance(ups, nn.ModuleList)\n",
    "            block1, block2, upsample = ups\n",
    "            yt = torch.cat((yt, h.pop()), dim=1)\n",
    "            yt = block1(yt, t_index)\n",
    "            yt = block2(yt, t_index)\n",
    "            yt = upsample(yt)\n",
    "\n",
    "        yt = torch.cat((yt, r), dim=1)\n",
    "\n",
    "        return self.final_conv(yt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe6cd89",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a6c28",
   "metadata": {},
   "source": [
    "### Define loss funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd43121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(torch.nn.Module, metaclass=abc.ABCMeta):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(\n",
    "        self, predicts: torch.Tensor, targets: torch.Tensor, masks: torch.Tensor\n",
    "    ):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "def make_loss(loss_name: str) -> CustomLoss:\n",
    "    if loss_name == \"L2\":\n",
    "        logger.info(\"L2 loss is created.\")\n",
    "        return L2Loss()\n",
    "    elif loss_name == \"L1\":\n",
    "        logger.info(\"L1 loss is created.\")\n",
    "        return L1Loss()\n",
    "    else:\n",
    "        raise ValueError(f\"{loss_name} is not supported.\")\n",
    "\n",
    "\n",
    "class L2Loss(CustomLoss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(\n",
    "        self, predicts: torch.Tensor, targets: torch.Tensor, masks: torch.Tensor\n",
    "    ):\n",
    "        return self.loss(predicts, targets)\n",
    "\n",
    "\n",
    "class L1Loss(CustomLoss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.loss = nn.L1Loss()\n",
    "\n",
    "    def forward(\n",
    "        self, predicts: torch.Tensor, targets: torch.Tensor, masks: torch.Tensor\n",
    "    ):\n",
    "        return self.loss(predicts, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79056ec5",
   "metadata": {},
   "source": [
    "### Construct diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e2b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(42)\n",
    "\n",
    "unet = Unet1D(\n",
    "    dim=config.n_features,\n",
    "    in_channels=32,  # num of times\n",
    "    out_channels=32,  # dont change in_channels and out_channels\n",
    "    padding_mode=\"circular\",\n",
    "    dim_mults=config.list_channel,\n",
    ").to(DEVICE)\n",
    "\n",
    "ddpm = DDPM(config=config.ddpm, neural_net=unet, device=torch.device(DEVICE))\n",
    "\n",
    "loss_fn = make_loss(loss_name=config.loss_name)\n",
    "optimizer = torch.optim.AdamW(ddpm.parameters(), lr=config.learning_rate)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365a964",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f4e9c",
   "metadata": {},
   "source": [
    "### Define training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0.0\n",
    "        self.avg = 0.0\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def optimize_ddpm(\n",
    "    *,\n",
    "    dataloader: DataLoader,\n",
    "    ddpm: DDPM,\n",
    "    loss_fn: CustomLoss,\n",
    "    optimizer: Optimizer,\n",
    "    epoch: int,\n",
    "    mode: typing.Literal[\"train\", \"valid\", \"test\"],\n",
    "    scaler: GradScaler,\n",
    "    use_amp: bool,\n",
    ") -> float:\n",
    "    #\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    d = next(ddpm.net.parameters()).device\n",
    "    device = str(d)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        ddpm.net.train()\n",
    "    elif mode in [\"valid\", \"test\"]:\n",
    "        ddpm.net.eval()\n",
    "    else:\n",
    "        raise ValueError(f\"{mode} is not supported.\")\n",
    "\n",
    "    random.seed(epoch)\n",
    "    np.random.seed(epoch)\n",
    "\n",
    "    device_type = \"cuda\" if \"cuda\" in device else \"cpu\"\n",
    "\n",
    "    for batch in dataloader:\n",
    "\n",
    "        for k in batch.keys():\n",
    "            batch[k] = batch[k].to(device, non_blocking=True)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(\n",
    "                device_type=device_type, dtype=torch.float16, enabled=use_amp\n",
    "            ):\n",
    "                noise, noise_hat = ddpm(**batch)\n",
    "                loss = loss_fn(predicts=noise_hat, targets=noise, masks=None)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad(), torch.autocast(\n",
    "                device_type=device_type, dtype=torch.float16, enabled=use_amp\n",
    "            ):\n",
    "                noise, noise_hat = ddpm(**batch)\n",
    "                loss = loss_fn(predicts=noise_hat, targets=noise, masks=None)\n",
    "\n",
    "        loss_meter.update(loss.item(), n=noise.shape[0])\n",
    "\n",
    "    return loss_meter.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca4e34",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42069ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "all_scores: list[dict] = []\n",
    "set_seeds(42)\n",
    "\n",
    "with tqdm(total=config.total_epochs, desc=\"Training Progress\", unit=\"step\") as pbar:\n",
    "    for _epoch in range(config.total_epochs):\n",
    "        epoch = _epoch + 1  # 0 から始まるため，1 を足す\n",
    "\n",
    "        loss = optimize_ddpm(\n",
    "            dataloader=dataloader,\n",
    "            ddpm=ddpm,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            mode=\"train\",\n",
    "            scaler=scaler,\n",
    "            use_amp=config.use_auto_mix_precision,\n",
    "        )\n",
    "        all_scores.append({\"epoch\": epoch, \"loss\": loss})\n",
    "\n",
    "        if epoch % config.save_interval == 0:\n",
    "            p = f\"{DL_RESULT_DIR}/model_weight_{epoch:06}.pth\"\n",
    "            torch.save(ddpm.net.state_dict(), p)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == config.total_epochs:\n",
    "            p = f\"{DL_RESULT_DIR}/loss_history.csv\"\n",
    "            pd.DataFrame(all_scores).to_csv(p, index=False)\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss})\n",
    "        pbar.update(1)\n",
    "\n",
    "end_time = time.time()\n",
    "logger.info(f\"Finished. Total elapsed time = {(end_time - start_time) / 60.} min\")\n",
    "del epoch, _epoch, loss, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f42f95",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bcc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = config.total_epochs\n",
    "p = f\"{DL_RESULT_DIR}/model_weight_{epoch:06}.pth\"\n",
    "unet.load_state_dict(torch.load(p, map_location=DEVICE, weights_only=False))\n",
    "_ = unet.eval()\n",
    "del epoch, p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded1ea49",
   "metadata": {},
   "source": [
    "## Run sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(42)\n",
    "dict_samples = ddpm.backward_sample_y(n_batches=50, n_return_steps=1000)\n",
    "# n_return_steps == 1000, つまり，1000 ステップを 1000 等分するように，中間状態を返す\n",
    "# この場合，1 ステップごとに返ってくる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e94e84",
   "metadata": {},
   "source": [
    "## Plot generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aee61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_samples = dict_samples[0]  # at t == 0\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "fig, axes = plt.subplots(1, 3, sharex=True, sharey=True, figsize=[10, 4])\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    d = last_samples[i].cpu().numpy()\n",
    "    d = dataloader.dataset.standardize_inversely(d)\n",
    "    ts = np.arange(d.shape[0]) * 0.2  # dt = 0.2\n",
    "    xs = np.linspace(0, 2 * math.pi, 32, endpoint=False)\n",
    "    X, T = np.meshgrid(xs, ts, indexing=\"ij\")\n",
    "    ret = ax.pcolormesh(\n",
    "        X, T, d.transpose(), vmin=-9, vmax=9, cmap=\"coolwarm\", shading=\"nearest\"\n",
    "    )\n",
    "    cbar = fig.colorbar(ret, ax=ax)\n",
    "    ax.set_xlabel(r\"Space, $x$\")\n",
    "    ax.set_ylabel(r\"Time, $t$\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "del d, ts, xs, X, T, ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995d9c5",
   "metadata": {},
   "source": [
    "## Plot intermediate states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556a991",
   "metadata": {},
   "source": [
    "- ノイズからデータの生成に成功している\n",
    "- これは非ガウスのデータ分布の学習に成功していることを意味する\n",
    "- 時間ステップが 0 に近づくと急にデータに空間構造が現れるのは，指数的に減衰する緩和の逆回しをしているため"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 12\n",
    "fig, axes = plt.subplots(2, 6, sharex=True, sharey=True, figsize=(15, 6))\n",
    "\n",
    "for t, ax in zip([1000, 500, 200, 100, 80, 60, 50, 40, 30, 20, 10, 0], axes.flatten()):\n",
    "    d = dict_samples[t][0].cpu().numpy()\n",
    "    d = dataloader.dataset.standardize_inversely(d)\n",
    "\n",
    "    ts = np.arange(d.shape[0]) * 0.2  # dt = 0.2\n",
    "    xs = np.linspace(0, 2 * math.pi, 32, endpoint=False)\n",
    "    X, T = np.meshgrid(xs, ts, indexing=\"ij\")\n",
    "\n",
    "    ret = ax.pcolormesh(\n",
    "        X, T, d.transpose(), vmin=-9, vmax=9, cmap=\"coolwarm\", shading=\"nearest\"\n",
    "    )\n",
    "\n",
    "    cbar = fig.colorbar(ret, ax=ax)\n",
    "    ax.set_xlabel(r\"Space, $x$\")\n",
    "    ax.set_ylabel(r\"Time, $t$\")\n",
    "    ax.set_title(f\"Diffusion Step = {t}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "del d, ts, xs, X, T, ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de8a162",
   "metadata": {},
   "source": [
    "## Run spectral analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7a99b",
   "metadata": {},
   "source": [
    "- Lorenz96 はカオス系なので，正解データに対して厳密な一致を確認するのは難しい\n",
    "- そこで，時空間スペクトルを比較し，訓練データと同様の時空間構造が生成されたデータに表れることを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08951c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.fft\n",
    "from scipy.signal import welch\n",
    "\n",
    "\n",
    "def compute_1d_psds_for_lorenz96(samples: np.ndarray, dt: float, dx: float):\n",
    "    assert isinstance(samples, np.ndarray)\n",
    "    (n_batches, n_times, n_spaces) = samples.shape\n",
    "\n",
    "    # Compute PSD in the time direction\n",
    "    freqs_time, psd_time = welch(\n",
    "        samples,\n",
    "        fs=1 / dt,\n",
    "        axis=1,\n",
    "        nperseg=n_times,\n",
    "        detrend=\"constant\",\n",
    "        scaling=\"density\",\n",
    "        window=\"hamming\",\n",
    "    )\n",
    "    psd_time_mean = psd_time.mean(axis=(0, 2))\n",
    "\n",
    "    # Compute PSD in the space direction\n",
    "    freqs_space, psd_space = welch(\n",
    "        samples,\n",
    "        fs=1 / dx,\n",
    "        axis=2,\n",
    "        nperseg=n_spaces,\n",
    "        detrend=\"constant\",\n",
    "        scaling=\"density\",\n",
    "        window=\"hamming\",\n",
    "    )\n",
    "    psd_space_mean = psd_space.mean(axis=(0, 1))\n",
    "\n",
    "    return freqs_time, psd_time_mean, freqs_space, psd_space_mean\n",
    "\n",
    "\n",
    "def compute_2d_psd_for_lorenz96(data: torch.Tensor) -> torch.Tensor:\n",
    "    assert isinstance(data, torch.Tensor) and data.ndim == 3\n",
    "    (n_batches, n_times, n_spaces) = data.shape\n",
    "\n",
    "    # Perform 2D FFT along time and space axes with norm='ortho'\n",
    "    fft_result = torch.fft.fft2(data, dim=(-2, -1), norm=\"ortho\")\n",
    "\n",
    "    # Compute PSD and shift zero frequency to center\n",
    "    psd = torch.abs(torch.fft.fftshift(fft_result, dim=(-2, -1))) ** 2\n",
    "\n",
    "    # Average PSD over batches\n",
    "    psd_mean = psd.mean(dim=0)\n",
    "\n",
    "    return psd_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_samples = torch.stack(\n",
    "    [dataloader.dataset[i][\"y0\"] for i in range(50)], dim=0\n",
    ").numpy()\n",
    "my_samples = dict_samples[0].cpu().numpy()  # at t == 0\n",
    "\n",
    "assert gt_samples.shape == my_samples.shape\n",
    "assert isinstance(gt_samples, np.ndarray) and isinstance(my_samples, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = dataloader.dataset.data[\"space\"].values\n",
    "ts = dataloader.dataset.data[\"time\"].values\n",
    "dt = float(np.mean(np.diff(ts.astype(np.float64))))\n",
    "dx = float(np.mean(np.diff(xs.astype(np.float64))))\n",
    "\n",
    "_, gt_psd_time_mean, _, gt_psd_space_mean = compute_1d_psds_for_lorenz96(\n",
    "    gt_samples, dt=dt, dx=dx\n",
    ")\n",
    "\n",
    "freqs_time, my_psd_time_mean, freqs_space, my_psd_space_mean = (\n",
    "    compute_1d_psds_for_lorenz96(my_samples, dt=dt, dx=dx)\n",
    ")\n",
    "\n",
    "gt_psd_2d_mean = compute_2d_psd_for_lorenz96(torch.from_numpy(gt_samples)).numpy()\n",
    "my_psd_2d_mean = compute_2d_psd_for_lorenz96(torch.from_numpy(my_samples)).numpy()\n",
    "\n",
    "del xs, ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.zeros(32)\n",
    "xs[-17:] = freqs_space\n",
    "xs[:15] = -np.flip(freqs_space[1:16])\n",
    "\n",
    "ys = np.zeros(32)\n",
    "ys[-17:] = freqs_time\n",
    "ys[:15] = -np.flip(freqs_time[1:16])\n",
    "\n",
    "xs, ys = np.meshgrid(xs, ys, indexing=\"ij\")\n",
    "vmin, vmax = None, None\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for ax, kind, data in zip(\n",
    "    axes, [\"Physics Simulation\", \"Langevin Sampling\"], [gt_psd_2d_mean, my_psd_2d_mean]\n",
    "):\n",
    "    d = data.transpose()  # time, space --> space, time\n",
    "\n",
    "    if vmin is None or vmax is None:\n",
    "        vmin, vmax = np.quantile(np.log10(d), [0.01, 0.99])\n",
    "    pm = ax.pcolormesh(xs, ys, np.log10(d), shading=\"nearest\", vmin=vmin, vmax=vmax)\n",
    "    ax.set_xlabel(r\"Wavenumber, $k$\")\n",
    "    ax.set_ylabel(r\"Frequency, $\\omega$\")\n",
    "\n",
    "    ax.set_title(kind)\n",
    "    ax.axvline(0, ls=\"--\", color=\"k\", alpha=0.5)\n",
    "    ax.axhline(0, ls=\"--\", color=\"k\", alpha=0.5)\n",
    "    fig.colorbar(pm, ax=ax, label=r\"$\\log_{10}(\\mathrm{PSD})$\", extend=\"both\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for kind in [\"Physics Simulation\", \"Langevin Sampling\"]:\n",
    "\n",
    "    for t, cmap in zip(\n",
    "        reversed([1000] + list(np.arange(480, 0, -20)) + [0]),\n",
    "        plt.cm.turbo(np.linspace(0, 1, 28))[1:-1],\n",
    "    ):\n",
    "        if kind == \"Physics Simulation\" and t != 0:\n",
    "            continue\n",
    "\n",
    "        ls, label, lw = \"--\", None, 1.5\n",
    "        if kind == \"Physics Simulation\":\n",
    "            lw = 4.0\n",
    "            cmap = \"k\"\n",
    "            ls = \"-\"\n",
    "            label = r\"Phys. Sim. ${\\rm Diffusion Time}=0.0$\"\n",
    "        else:\n",
    "            if t == 1000:\n",
    "                lw = 4.0\n",
    "                label = r\"Langevin ${\\rm Diffusion Time}=1.0$\"\n",
    "            elif t == 0:\n",
    "                lw = 4.0\n",
    "                label = r\"Langevin ${\\rm Diffusion Time}=0.0$\"\n",
    "\n",
    "            _freqs_time, psd_time_mean, _freqs_space, psd_space_mean = (\n",
    "                compute_1d_psds_for_lorenz96(\n",
    "                    dict_samples[t].squeeze().numpy(), dt=dt, dx=dx\n",
    "                )\n",
    "            )\n",
    "            assert np.all(_freqs_space == freqs_space)\n",
    "            assert np.all(_freqs_time == freqs_time)\n",
    "\n",
    "        d = gt_psd_time_mean if kind == \"Physics Simulation\" else psd_time_mean\n",
    "        axes[0].plot(freqs_time, d, lw=lw, color=cmap, ls=ls, label=label)\n",
    "        axes[0].set_title(\"PSD in Time Direction\")\n",
    "        axes[0].set_xlabel(r\"Frequency, $\\omega$ (1/time)\")\n",
    "        axes[0].set_ylabel(\"Power Spectral Density\")\n",
    "        axes[0].set_xscale(\"log\")\n",
    "        axes[0].set_yscale(\"log\")\n",
    "        axes[0].legend(loc=\"lower left\", fontsize=15)\n",
    "\n",
    "        d = gt_psd_space_mean if kind == \"Physics Simulation\" else psd_space_mean\n",
    "        axes[1].plot(freqs_space, d, lw=lw, color=cmap, ls=ls, label=label)\n",
    "        axes[1].set_title(\"PSD in Space Direction\")\n",
    "        axes[1].set_xlabel(r\"Wavenumber, $k$ (1/space)\")\n",
    "        axes[1].set_ylabel(\"Power Spectral Density\")\n",
    "        axes[1].set_xscale(\"log\")\n",
    "        axes[1].set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del xs, ys, d\n",
    "del _freqs_time, psd_time_mean, _freqs_space, psd_space_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864653c",
   "metadata": {},
   "source": [
    "- ノイズによりエネルギーが注入され，スペクトルが平坦に近づく\n",
    "- 逆過程では，その平坦なスペクトルから初めて，段々と時空間構造を復元する\n",
    "- ノイズは，全ての格子点で独立に作用し，データの時空間構造を一切加味することなく破壊する\n",
    "- そのため，エネルギーが平均より大きいスケールでエネルギーの注入が起こり，逆にエネルギーが小さいスケールでエネルギーの減衰が起こる\n",
    "- この注入と減衰は，スケールの大きさではなくて，エネルギーの大きさで決定される\n",
    "- 例えば，空間スペクトルを見ると，$k$ の小さな領域と大きな領域でエネルギーが注入されている"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211d580",
   "metadata": {},
   "source": [
    "# Run Score-based DA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbbbfb",
   "metadata": {},
   "source": [
    "- ここでは観測データを正解にノイズを加えて作る\n",
    "- そしてこの観測データを条件とする生成を行う\n",
    "  - この条件付き生成により観測データの同化が実現される\n",
    "- なお簡単のため共分散行列は対角としている\n",
    "  - そのため，欠損のない観測データを考えている"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debdf88c",
   "metadata": {},
   "source": [
    "## Define sda class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreBasedDA(DDPM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DDPMConfig,\n",
    "        neural_net: torch.nn.Module,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        super().__init__(config=config, neural_net=neural_net, device=device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_mean_for_likelihood(\n",
    "        self, *, yt: torch.Tensor, t_index: torch.Tensor, score: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        decay = self._extract_params(self.decays, t_index)\n",
    "        std = self._extract_params(self.stds, t_index)\n",
    "\n",
    "        mean = (yt + (std**2) * score) / decay\n",
    "\n",
    "        return mean\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_var_for_likelihood(\n",
    "        self, *, t_index: torch.Tensor, dsdx: float, std_for_obs: float\n",
    "    ) -> torch.Tensor:\n",
    "        decay = self._extract_params(self.decays, t_index)\n",
    "        std = self._extract_params(self.stds, t_index)\n",
    "\n",
    "        vars = std_for_obs**2 + (std**2 + (std**4) * dsdx) / (decay**2)\n",
    "\n",
    "        return vars\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_derivative_of_likelihood(\n",
    "        self,\n",
    "        *,\n",
    "        yt: torch.Tensor,\n",
    "        t_index: torch.Tensor,\n",
    "        dsdx: float,\n",
    "        obs: torch.Tensor,\n",
    "        std_for_obs: float,\n",
    "        score: torch.Tensor,\n",
    "    ):\n",
    "        mean = self._get_mean_for_likelihood(yt=yt, t_index=t_index, score=score)\n",
    "        var = self._get_var_for_likelihood(\n",
    "            t_index=t_index, dsdx=dsdx, std_for_obs=std_for_obs\n",
    "        )\n",
    "        decay = self._extract_params(self.decays, t_index)\n",
    "\n",
    "        o = obs.to(mean.device)\n",
    "        derivatives = (o - mean) / var / decay\n",
    "        masked = torch.where(torch.isnan(o), torch.zeros_like(o), derivatives)\n",
    "\n",
    "        return masked\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _backward_sample_y_with_assimilation(\n",
    "        self,\n",
    "        *,\n",
    "        yt: torch.Tensor,\n",
    "        t_index: torch.Tensor,\n",
    "        dsdx: float,\n",
    "        obs: torch.Tensor,\n",
    "        std_for_obs: float,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        friction = self._extract_params(self.frictions, t_index)\n",
    "        sigma = self._extract_params(self.sigmas, t_index)\n",
    "        std = self._extract_params(self.stds, t_index)\n",
    "        t = self._extract_params(self.times, t_index, for_broadcast=False)\n",
    "        t = t[:, None]  # add channel dim\n",
    "\n",
    "        est_noise = self.net(yt=yt, t=t, t_index=t_index, y_cond=None)\n",
    "        score = -est_noise / std\n",
    "        dldx = self._get_derivative_of_likelihood(\n",
    "            yt=yt,\n",
    "            t_index=t_index,\n",
    "            dsdx=dsdx,\n",
    "            obs=obs,\n",
    "            std_for_obs=std_for_obs,\n",
    "            score=score,\n",
    "        )\n",
    "\n",
    "        mean = yt + self.dt * (friction * yt + (sigma**2) * (score + dldx))\n",
    "        dW = self.sqrt_dt * torch.randn_like(yt)\n",
    "\n",
    "        n_batches = yt.shape[0]\n",
    "        mask = (1 - (t_index == 0).float()).reshape(n_batches, *((1,) * (yt.ndim - 1)))\n",
    "        mask = mask.to(dtype=self.dtype, device=self.device)\n",
    "        # no noise at t_index == 0\n",
    "\n",
    "        return mean + mask * sigma * dW\n",
    "\n",
    "    # public method\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def assimilate(\n",
    "        self,\n",
    "        *,\n",
    "        n_batches: int,\n",
    "        derivative_score: float,\n",
    "        observations: torch.Tensor,\n",
    "        std_for_observations: float,\n",
    "        n_return_steps: Optional[int] = None,\n",
    "        tqdm_disable: bool = False,\n",
    "    ):\n",
    "        assert not self.net.training\n",
    "        assert observations.shape == (self.c.n_channels, self.c.n_spaces)\n",
    "\n",
    "        size = (n_batches, self.c.n_channels, self.c.n_spaces)\n",
    "        yt = torch.randn(size=size, device=self.device)\n",
    "        yt = self.stds[-1] * yt\n",
    "\n",
    "        if n_return_steps is not None:\n",
    "            interval = self.c.n_timesteps // n_return_steps\n",
    "\n",
    "        intermidiates: dict[int, torch.Tensor] = {}\n",
    "\n",
    "        for i in tqdm(\n",
    "            reversed(range(0, self.c.n_timesteps)),\n",
    "            total=self.c.n_timesteps,\n",
    "            disable=tqdm_disable,\n",
    "        ):\n",
    "            if interval is not None and (i + 1) % interval == 0:\n",
    "                intermidiates[i + 1] = yt.detach().clone().cpu()\n",
    "\n",
    "            index = torch.full((n_batches,), i, device=self.device, dtype=torch.long)\n",
    "            yt = self._backward_sample_y_with_assimilation(\n",
    "                yt=yt,\n",
    "                t_index=index,\n",
    "                dsdx=derivative_score,\n",
    "                obs=observations,\n",
    "                std_for_obs=std_for_observations,\n",
    "            )\n",
    "\n",
    "        intermidiates[0] = yt.detach().clone().cpu()\n",
    "\n",
    "        return intermidiates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93309af9",
   "metadata": {},
   "source": [
    "## Make a sda instance and load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd2f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sda = ScoreBasedDA(\n",
    "    config=config.ddpm,\n",
    "    neural_net=unet,\n",
    "    device=torch.device(DEVICE),\n",
    ")\n",
    "\n",
    "epoch = config.total_epochs\n",
    "p = f\"{DL_RESULT_DIR}/model_weight_{epoch:06}.pth\"\n",
    "unet.load_state_dict(torch.load(p, map_location=DEVICE, weights_only=False))\n",
    "_ = unet.eval()\n",
    "del epoch, p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97741978",
   "metadata": {},
   "source": [
    "## Make observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb59ec",
   "metadata": {},
   "source": [
    "- ノイズは測定誤差を模倣したもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f3e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = dataloader.dataset[-1][\"y0\"]\n",
    "obs = torch.full_like(ground_truth, torch.nan)\n",
    "obs = (\n",
    "    ground_truth + torch.randn_like(obs) * ground_truth.abs().mean() * 0.1\n",
    ")  # add measurement noise\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, sharey=True, figsize=[10, 4])\n",
    "\n",
    "for ax, data, ttl in zip(\n",
    "    axes.flatten(), [ground_truth, obs], [\"ground truth\", \"observations\"]\n",
    "):\n",
    "    d = data.cpu().numpy()\n",
    "    d = dataloader.dataset.standardize_inversely(d)\n",
    "    ts = np.arange(d.shape[0]) * 0.2  # dt = 0.2\n",
    "    xs = np.linspace(0, 2 * math.pi, 32, endpoint=False)\n",
    "    X, T = np.meshgrid(xs, ts, indexing=\"ij\")\n",
    "    ret = ax.pcolormesh(\n",
    "        X, T, d.transpose(), vmin=-9, vmax=9, cmap=\"coolwarm\", shading=\"nearest\"\n",
    "    )\n",
    "    cbar = fig.colorbar(ret, ax=ax)\n",
    "    ax.set_xlabel(r\"Space, $x$\")\n",
    "    ax.set_ylabel(r\"Time, $t$\")\n",
    "    ax.set_title(ttl)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "del d, ts, xs, X, T, ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a32c68",
   "metadata": {},
   "source": [
    "## Perform assimilation with observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16233c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(42)\n",
    "dict_samples_with_obs = sda.assimilate(\n",
    "    n_batches=10,\n",
    "    derivative_score=0.0,\n",
    "    observations=obs,\n",
    "    std_for_observations=0.1,\n",
    "    n_return_steps=1000,\n",
    ")\n",
    "# n_return_steps == 1000, つまり，1000 ステップを 1000 等分するように，中間状態を返す\n",
    "# この場合，1 ステップごとに返ってくる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d14352",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_samples = dict_samples_with_obs[0]  # at t == 0\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "fig, axes = plt.subplots(2, 4, sharex=True, sharey=True, figsize=[15, 6])\n",
    "\n",
    "gt = None\n",
    "for i in range(axes.shape[1]):\n",
    "    ax = axes[0, i]\n",
    "\n",
    "    if i == 0:\n",
    "        d = ground_truth.cpu().numpy()\n",
    "        ttl = \"Ground Truth (GT)\"\n",
    "    else:\n",
    "        d = last_samples[i].cpu().numpy()\n",
    "        ttl = f\"Sample {i}\"\n",
    "    d = (dataloader.dataset.standardize_inversely(d)).transpose()\n",
    "\n",
    "    ts = np.arange(d.shape[0]) * 0.2  # dt = 0.2\n",
    "    xs = np.linspace(0, 2 * math.pi, 32, endpoint=False)\n",
    "    X, T = np.meshgrid(xs, ts, indexing=\"ij\")\n",
    "\n",
    "    ret = ax.pcolormesh(X, T, d, vmin=-10, vmax=10, cmap=\"coolwarm\", shading=\"nearest\")\n",
    "    cbar = fig.colorbar(ret, ax=ax)\n",
    "    ax.set_xlabel(r\"Space, $x$\")\n",
    "    ax.set_ylabel(r\"Time, $t$\")\n",
    "    ax.set_title(ttl)\n",
    "\n",
    "    if i == 0:\n",
    "        gt = d.copy()\n",
    "        ax = axes[1, i]\n",
    "        d = obs.numpy().transpose()\n",
    "        d = dataloader.dataset.standardize_inversely(d)\n",
    "        cmap = \"coolwarm\"\n",
    "        vmin, vmax = -10, 10\n",
    "        ttl = \"Observations\"\n",
    "    else:\n",
    "        d = gt - d\n",
    "        cmap = \"PiYG\"\n",
    "        vmin, vmax = -5, 5\n",
    "        ttl = f\"GT - Sample {i}\"\n",
    "\n",
    "    ax = axes[1, i]\n",
    "    ret = ax.pcolormesh(X, T, d, vmin=vmin, vmax=vmax, cmap=cmap, shading=\"nearest\")\n",
    "    cbar = fig.colorbar(ret, ax=ax)\n",
    "    ax.set_xlabel(r\"Space, $x$\")\n",
    "    ax.set_ylabel(r\"Time, $t$\")\n",
    "    ax.set_title(ttl)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "del gt, d, ts, xs, X, T, ret, last_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a64a3b",
   "metadata": {},
   "source": [
    "- 条件付き生成により，正解と同様のデータが生成できている"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797bf31",
   "metadata": {},
   "source": [
    "## Case without observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaabd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(42)\n",
    "dict_samples_without_obs = sda.assimilate(\n",
    "    n_batches=10,\n",
    "    derivative_score=0.0,\n",
    "    observations=torch.full_like(obs, fill_value=torch.nan),  # 観測値は全て欠損\n",
    "    std_for_observations=0.1,\n",
    "    n_return_steps=1000,\n",
    ")\n",
    "# n_return_steps == 1000, つまり，1000 ステップを 1000 等分するように，中間状態を返す\n",
    "# この場合，1 ステップごとに返ってくる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc533f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_samples = dict_samples_without_obs[0]  # at t == 0\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "fig, axes = plt.subplots(2, 4, sharex=True, sharey=True, figsize=[15, 6])\n",
    "\n",
    "gt = None\n",
    "for i in range(axes.shape[1]):\n",
    "    ax = axes[0, i]\n",
    "\n",
    "    if i == 0:\n",
    "        d = ground_truth.cpu().numpy()\n",
    "        ttl = \"Ground Truth (GT)\"\n",
    "    else:\n",
    "        d = last_samples[i].cpu().numpy()\n",
    "        ttl = f\"Sample {i}\"\n",
    "    d = (dataloader.dataset.standardize_inversely(d)).transpose()\n",
    "\n",
    "    ts = np.arange(d.shape[0]) * 0.2  # dt = 0.2\n",
    "    xs = np.linspace(0, 2 * math.pi, 32, endpoint=False)\n",
    "    X, T = np.meshgrid(xs, ts, indexing=\"ij\")\n",
    "\n",
    "    ret = ax.pcolormesh(X, T, d, vmin=-10, vmax=10, cmap=\"coolwarm\", shading=\"nearest\")\n",
    "    cbar = fig.colorbar(ret, ax=ax)\n",
    "    ax.set_xlabel(r\"Space, $x$\")\n",
    "    ax.set_ylabel(r\"Time, $t$\")\n",
    "    ax.set_title(ttl)\n",
    "\n",
    "    if i == 0:\n",
    "        gt = d.copy()\n",
    "        ax = axes[1, i]\n",
    "        d = obs.numpy().transpose()\n",
    "        d = dataloader.dataset.standardize_inversely(d)\n",
    "        cmap = \"coolwarm\"\n",
    "        vmin, vmax = -10, 10\n",
    "        ttl = \"Observations\"\n",
    "    else:\n",
    "        d = gt - d\n",
    "        cmap = \"PiYG\"\n",
    "        vmin, vmax = -5, 5\n",
    "        ttl = f\"GT - Sample {i}\"\n",
    "\n",
    "    ax = axes[1, i]\n",
    "    ret = ax.pcolormesh(X, T, d, vmin=vmin, vmax=vmax, cmap=cmap, shading=\"nearest\")\n",
    "    cbar = fig.colorbar(ret, ax=ax)\n",
    "    ax.set_xlabel(r\"Space, $x$\")\n",
    "    ax.set_ylabel(r\"Time, $t$\")\n",
    "    ax.set_title(ttl)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "del gt, d, ts, xs, X, T, ret, last_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be107c3e",
   "metadata": {},
   "source": [
    "- 観測データがない場合 (つまり無条件の生成の場合)，得られるサンプルは正解とは大きく異なる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fcd07",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
